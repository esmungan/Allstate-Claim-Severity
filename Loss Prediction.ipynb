{"cells":[{"cell_type":"markdown","source":["<h2>Allstate Claim Severity Kaggle Project</h2>\n\nThis goal of this project is to be able to predict the cost (and hence severity) of a given insurance claim for Allstate insurance company.\n\nThe dataset and project is outlined in this Kaggle project below:\n- https://www.kaggle.com/c/allstate-claims-severity/data\n\nwhereas the mean absolute error is the evaluation metric.\n\n\n\n**_Data Structure_**\n\nEach row in this dataset represents an insurance claim. The values in 'loss' column should be predicted. Variables prefaced with 'cat' are categorical, while those prefaced with 'cont' are continuous."],"metadata":{}},{"cell_type":"markdown","source":["<h3>Data Analysis</h3>"],"metadata":{}},{"cell_type":"code","source":["display(dbutils.fs.ls(\"dbfs:/FileStore/tables/allstate/\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/allstate/allstate_test.csv</td><td>allstate_test.csv</td><td>45715862</td></tr><tr><td>dbfs:/FileStore/tables/allstate/allstate_train.csv</td><td>allstate_train.csv</td><td>70025339</td></tr></tbody></table></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#Let us first load the data\n\ndataset = (spark.read\n .option(\"header\",\"true\")\n .option(\"inferSchema\",\"true\")\n .csv(\"dbfs:/FileStore/tables/allstate/allstate_train.csv\")\n .cache()\n             )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["print(\"Data set is a '%.5e' by '%i' matrix \\n\" \n      %(dataset.count(),len(dataset.columns)))         #Check the size of the dta matrix\nprint(dataset.columns,\"\\n\")                            \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Data set is a &#39;1.88317e+05&#39; by &#39;133&#39; matrix \n\n[&#39;id&#39;, &#39;cat1&#39;, &#39;cat2&#39;, &#39;cat3&#39;, &#39;cat4&#39;, &#39;cat5&#39;, &#39;cat6&#39;, &#39;cat7&#39;, &#39;cat8&#39;, &#39;cat9&#39;, &#39;cat10&#39;, &#39;cat11&#39;, &#39;cat12&#39;, &#39;cat13&#39;, &#39;cat14&#39;, &#39;cat15&#39;, &#39;cat16&#39;, &#39;cat17&#39;, &#39;cat18&#39;, &#39;cat19&#39;, &#39;cat20&#39;, &#39;cat21&#39;, &#39;cat22&#39;, &#39;cat23&#39;, &#39;cat24&#39;, &#39;cat25&#39;, &#39;cat26&#39;, &#39;cat27&#39;, &#39;cat28&#39;, &#39;cat29&#39;, &#39;cat30&#39;, &#39;cat31&#39;, &#39;cat32&#39;, &#39;cat33&#39;, &#39;cat34&#39;, &#39;cat35&#39;, &#39;cat36&#39;, &#39;cat37&#39;, &#39;cat38&#39;, &#39;cat39&#39;, &#39;cat40&#39;, &#39;cat41&#39;, &#39;cat42&#39;, &#39;cat43&#39;, &#39;cat44&#39;, &#39;cat45&#39;, &#39;cat46&#39;, &#39;cat47&#39;, &#39;cat48&#39;, &#39;cat49&#39;, &#39;cat50&#39;, &#39;cat51&#39;, &#39;cat52&#39;, &#39;cat53&#39;, &#39;cat54&#39;, &#39;cat55&#39;, &#39;cat56&#39;, &#39;cat57&#39;, &#39;cat58&#39;, &#39;cat59&#39;, &#39;cat60&#39;, &#39;cat61&#39;, &#39;cat62&#39;, &#39;cat63&#39;, &#39;cat64&#39;, &#39;cat65&#39;, &#39;cat66&#39;, &#39;cat67&#39;, &#39;cat68&#39;, &#39;cat69&#39;, &#39;cat70&#39;, &#39;cat71&#39;, &#39;cat72&#39;, &#39;cat73&#39;, &#39;cat74&#39;, &#39;cat75&#39;, &#39;cat76&#39;, &#39;cat77&#39;, &#39;cat78&#39;, &#39;cat79&#39;, &#39;cat80&#39;, &#39;cat81&#39;, &#39;cat82&#39;, &#39;cat83&#39;, &#39;cat84&#39;, &#39;cat85&#39;, &#39;cat86&#39;, &#39;cat87&#39;, &#39;cat88&#39;, &#39;cat89&#39;, &#39;cat90&#39;, &#39;cat91&#39;, &#39;cat92&#39;, &#39;cat93&#39;, &#39;cat94&#39;, &#39;cat95&#39;, &#39;cat96&#39;, &#39;cat97&#39;, &#39;cat98&#39;, &#39;cat99&#39;, &#39;cat100&#39;, &#39;cat101&#39;, &#39;cat102&#39;, &#39;cat103&#39;, &#39;cat104&#39;, &#39;cat105&#39;, &#39;cat106&#39;, &#39;cat107&#39;, &#39;cat108&#39;, &#39;cat109&#39;, &#39;cat110&#39;, &#39;cat111&#39;, &#39;cat112&#39;, &#39;cat113&#39;, &#39;cat114&#39;, &#39;cat115&#39;, &#39;cat116&#39;, &#39;cont1&#39;, &#39;cont2&#39;, &#39;cont3&#39;, &#39;cont4&#39;, &#39;cont5&#39;, &#39;cont6&#39;, &#39;cont7&#39;, &#39;cont8&#39;, &#39;cont9&#39;, &#39;cont10&#39;, &#39;cont11&#39;, &#39;cont12&#39;, &#39;cont13&#39;, &#39;cont14&#39;, &#39;loss&#39;, &#39;log_loss&#39;] \n\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["We have medium sized labeled data with the following data structure:\n* \"id, 116 categorical, 14 continuous data and the loss (as label)\""],"metadata":{}},{"cell_type":"code","source":["dataset_nonull = dataset.dropDuplicates().na.drop()            #Remove any duplicates and null content\n\nprint(\"After removing null and replicate entries the data set is a '%.5e' by '%i' matrix \\n\" \n      %(dataset.count(),len(dataset.columns)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">After removing null and replicate entries the data set is a &#39;1.88317e+05&#39; by &#39;133&#39; matrix \n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["* The dataset size doesn't change after dropping duplicates and null content. Hence, we deem this dataset as a clean one"],"metadata":{}},{"cell_type":"code","source":["#Let us take a quick look at the categorical data\ndataset.select(dataset.columns[1:117]).toPandas()[0:10]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat1</th>\n      <th>cat2</th>\n      <th>cat3</th>\n      <th>cat4</th>\n      <th>cat5</th>\n      <th>cat6</th>\n      <th>cat7</th>\n      <th>cat8</th>\n      <th>cat9</th>\n      <th>cat10</th>\n      <th>cat11</th>\n      <th>cat12</th>\n      <th>cat13</th>\n      <th>cat14</th>\n      <th>cat15</th>\n      <th>cat16</th>\n      <th>cat17</th>\n      <th>cat18</th>\n      <th>cat19</th>\n      <th>cat20</th>\n      <th>cat21</th>\n      <th>cat22</th>\n      <th>cat23</th>\n      <th>cat24</th>\n      <th>cat25</th>\n      <th>cat26</th>\n      <th>cat27</th>\n      <th>cat28</th>\n      <th>cat29</th>\n      <th>cat30</th>\n      <th>cat31</th>\n      <th>cat32</th>\n      <th>cat33</th>\n      <th>cat34</th>\n      <th>cat35</th>\n      <th>cat36</th>\n      <th>cat37</th>\n      <th>cat38</th>\n      <th>cat39</th>\n      <th>cat40</th>\n      <th>...</th>\n      <th>cat77</th>\n      <th>cat78</th>\n      <th>cat79</th>\n      <th>cat80</th>\n      <th>cat81</th>\n      <th>cat82</th>\n      <th>cat83</th>\n      <th>cat84</th>\n      <th>cat85</th>\n      <th>cat86</th>\n      <th>cat87</th>\n      <th>cat88</th>\n      <th>cat89</th>\n      <th>cat90</th>\n      <th>cat91</th>\n      <th>cat92</th>\n      <th>cat93</th>\n      <th>cat94</th>\n      <th>cat95</th>\n      <th>cat96</th>\n      <th>cat97</th>\n      <th>cat98</th>\n      <th>cat99</th>\n      <th>cat100</th>\n      <th>cat101</th>\n      <th>cat102</th>\n      <th>cat103</th>\n      <th>cat104</th>\n      <th>cat105</th>\n      <th>cat106</th>\n      <th>cat107</th>\n      <th>cat108</th>\n      <th>cat109</th>\n      <th>cat110</th>\n      <th>cat111</th>\n      <th>cat112</th>\n      <th>cat113</th>\n      <th>cat114</th>\n      <th>cat115</th>\n      <th>cat116</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>D</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>B</td>\n      <td>C</td>\n      <td>E</td>\n      <td>A</td>\n      <td>C</td>\n      <td>T</td>\n      <td>B</td>\n      <td>G</td>\n      <td>A</td>\n      <td>A</td>\n      <td>I</td>\n      <td>E</td>\n      <td>G</td>\n      <td>J</td>\n      <td>G</td>\n      <td>BU</td>\n      <td>BC</td>\n      <td>C</td>\n      <td>AS</td>\n      <td>S</td>\n      <td>A</td>\n      <td>O</td>\n      <td>LB</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>D</td>\n      <td>D</td>\n      <td>A</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>D</td>\n      <td>C</td>\n      <td>E</td>\n      <td>E</td>\n      <td>D</td>\n      <td>T</td>\n      <td>L</td>\n      <td>F</td>\n      <td>A</td>\n      <td>A</td>\n      <td>E</td>\n      <td>E</td>\n      <td>I</td>\n      <td>K</td>\n      <td>K</td>\n      <td>BI</td>\n      <td>CQ</td>\n      <td>A</td>\n      <td>AV</td>\n      <td>BM</td>\n      <td>A</td>\n      <td>O</td>\n      <td>DP</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>D</td>\n      <td>C</td>\n      <td>E</td>\n      <td>E</td>\n      <td>A</td>\n      <td>D</td>\n      <td>L</td>\n      <td>O</td>\n      <td>A</td>\n      <td>B</td>\n      <td>E</td>\n      <td>F</td>\n      <td>H</td>\n      <td>F</td>\n      <td>A</td>\n      <td>AB</td>\n      <td>DK</td>\n      <td>A</td>\n      <td>C</td>\n      <td>AF</td>\n      <td>A</td>\n      <td>I</td>\n      <td>GK</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>D</td>\n      <td>D</td>\n      <td>D</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>D</td>\n      <td>C</td>\n      <td>E</td>\n      <td>E</td>\n      <td>D</td>\n      <td>T</td>\n      <td>I</td>\n      <td>D</td>\n      <td>A</td>\n      <td>A</td>\n      <td>E</td>\n      <td>E</td>\n      <td>I</td>\n      <td>K</td>\n      <td>K</td>\n      <td>BI</td>\n      <td>CS</td>\n      <td>C</td>\n      <td>N</td>\n      <td>AE</td>\n      <td>A</td>\n      <td>O</td>\n      <td>DJ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>H</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>E</td>\n      <td>E</td>\n      <td>A</td>\n      <td>P</td>\n      <td>F</td>\n      <td>J</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>E</td>\n      <td>K</td>\n      <td>G</td>\n      <td>B</td>\n      <td>H</td>\n      <td>C</td>\n      <td>C</td>\n      <td>Y</td>\n      <td>BM</td>\n      <td>A</td>\n      <td>K</td>\n      <td>CK</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>D</td>\n      <td>D</td>\n      <td>E</td>\n      <td>C</td>\n      <td>A</td>\n      <td>P</td>\n      <td>J</td>\n      <td>D</td>\n      <td>A</td>\n      <td>A</td>\n      <td>E</td>\n      <td>E</td>\n      <td>H</td>\n      <td>F</td>\n      <td>B</td>\n      <td>BI</td>\n      <td>CS</td>\n      <td>A</td>\n      <td>AS</td>\n      <td>AE</td>\n      <td>A</td>\n      <td>K</td>\n      <td>DJ</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>D</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>C</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>D</td>\n      <td>D</td>\n      <td>E</td>\n      <td>C</td>\n      <td>A</td>\n      <td>P</td>\n      <td>J</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>E</td>\n      <td>E</td>\n      <td>H</td>\n      <td>F</td>\n      <td>B</td>\n      <td>BI</td>\n      <td>DK</td>\n      <td>A</td>\n      <td>J</td>\n      <td>AF</td>\n      <td>A</td>\n      <td>K</td>\n      <td>DJ</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>A</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>C</td>\n      <td>D</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>C</td>\n      <td>B</td>\n      <td>C</td>\n      <td>E</td>\n      <td>A</td>\n      <td>C</td>\n      <td>T</td>\n      <td>H</td>\n      <td>C</td>\n      <td>A</td>\n      <td>A</td>\n      <td>K</td>\n      <td>F</td>\n      <td>F</td>\n      <td>I</td>\n      <td>G</td>\n      <td>BI</td>\n      <td>EB</td>\n      <td>G</td>\n      <td>AH</td>\n      <td>Y</td>\n      <td>A</td>\n      <td>P</td>\n      <td>LO</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>D</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>D</td>\n      <td>C</td>\n      <td>C</td>\n      <td>E</td>\n      <td>C</td>\n      <td>D</td>\n      <td>T</td>\n      <td>C</td>\n      <td>Q</td>\n      <td>A</td>\n      <td>C</td>\n      <td>H</td>\n      <td>F</td>\n      <td>G</td>\n      <td>M</td>\n      <td>K</td>\n      <td>BI</td>\n      <td>BC</td>\n      <td>C</td>\n      <td>K</td>\n      <td>AX</td>\n      <td>A</td>\n      <td>Q</td>\n      <td>IE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>A</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>...</td>\n      <td>D</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>B</td>\n      <td>C</td>\n      <td>B</td>\n      <td>D</td>\n      <td>C</td>\n      <td>D</td>\n      <td>A</td>\n      <td>A</td>\n      <td>B</td>\n      <td>H</td>\n      <td>D</td>\n      <td>B</td>\n      <td>C</td>\n      <td>E</td>\n      <td>A</td>\n      <td>C</td>\n      <td>T</td>\n      <td>F</td>\n      <td>D</td>\n      <td>A</td>\n      <td>F</td>\n      <td>K</td>\n      <td>H</td>\n      <td>G</td>\n      <td>J</td>\n      <td>G</td>\n      <td>BU</td>\n      <td>DW</td>\n      <td>A</td>\n      <td>U</td>\n      <td>S</td>\n      <td>J</td>\n      <td>O</td>\n      <td>LY</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 116 columns</p>\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["* The categorical data have been obfuscated. Hence it is not possible to make contextual deductions from the set.\n* Also, potentially , there are more distinct categories for data after cat109"],"metadata":{}},{"cell_type":"code","source":["#Let us now take a quick look at the continuous data\ndataset.select(dataset.columns[117:132]).describe().toPandas().transpose()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>summary</th>\n      <td>count</td>\n      <td>mean</td>\n      <td>stddev</td>\n      <td>min</td>\n      <td>max</td>\n    </tr>\n    <tr>\n      <th>cont1</th>\n      <td>188318</td>\n      <td>0.4938613645642003</td>\n      <td>0.1876401764138863</td>\n      <td>1.6E-5</td>\n      <td>0.984975</td>\n    </tr>\n    <tr>\n      <th>cont2</th>\n      <td>188318</td>\n      <td>0.507188356179494</td>\n      <td>0.20720173860981372</td>\n      <td>0.001149</td>\n      <td>0.862654</td>\n    </tr>\n    <tr>\n      <th>cont3</th>\n      <td>188318</td>\n      <td>0.4989184507216597</td>\n      <td>0.20210460819343745</td>\n      <td>0.002634</td>\n      <td>0.944251</td>\n    </tr>\n    <tr>\n      <th>cont4</th>\n      <td>188318</td>\n      <td>0.4918123025892123</td>\n      <td>0.21129221269283555</td>\n      <td>0.176921</td>\n      <td>0.954297</td>\n    </tr>\n    <tr>\n      <th>cont5</th>\n      <td>188318</td>\n      <td>0.4874277287832749</td>\n      <td>0.20902682854450408</td>\n      <td>0.281143</td>\n      <td>0.983674</td>\n    </tr>\n    <tr>\n      <th>cont6</th>\n      <td>188318</td>\n      <td>0.4909445337355028</td>\n      <td>0.20527256983553044</td>\n      <td>0.012683</td>\n      <td>0.997162</td>\n    </tr>\n    <tr>\n      <th>cont7</th>\n      <td>188318</td>\n      <td>0.4849702050680173</td>\n      <td>0.17845016396070937</td>\n      <td>0.069503</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>cont8</th>\n      <td>188318</td>\n      <td>0.4864373158699439</td>\n      <td>0.19937045456133265</td>\n      <td>0.23688</td>\n      <td>0.9802</td>\n    </tr>\n    <tr>\n      <th>cont9</th>\n      <td>188318</td>\n      <td>0.485506319895067</td>\n      <td>0.18166017135075585</td>\n      <td>8.0E-5</td>\n      <td>0.9954</td>\n    </tr>\n    <tr>\n      <th>cont10</th>\n      <td>188318</td>\n      <td>0.49806585042322377</td>\n      <td>0.18587672593201843</td>\n      <td>0.0</td>\n      <td>0.99498</td>\n    </tr>\n    <tr>\n      <th>cont11</th>\n      <td>188318</td>\n      <td>0.4935110085546688</td>\n      <td>0.2097365114474782</td>\n      <td>0.035321</td>\n      <td>0.998742</td>\n    </tr>\n    <tr>\n      <th>cont12</th>\n      <td>188318</td>\n      <td>0.49315042562582045</td>\n      <td>0.209426621076029</td>\n      <td>0.036232</td>\n      <td>0.998484</td>\n    </tr>\n    <tr>\n      <th>cont13</th>\n      <td>188318</td>\n      <td>0.49313761583599725</td>\n      <td>0.21277724232241005</td>\n      <td>2.28E-4</td>\n      <td>0.988494</td>\n    </tr>\n    <tr>\n      <th>cont14</th>\n      <td>188318</td>\n      <td>0.49571701797491147</td>\n      <td>0.22248753955922576</td>\n      <td>0.179722</td>\n      <td>0.844848</td>\n    </tr>\n    <tr>\n      <th>loss</th>\n      <td>188318</td>\n      <td>3037.3376856699924</td>\n      <td>2904.0861863904033</td>\n      <td>0.67</td>\n      <td>121012.25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["* The continuous data has already been normalized and have positive values\n* Loss values have a wide range and is potentially skewed, needs further investigation"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfrom math import log\n\nnpts = 10000\nloss_samples = dataset.select('loss').take(npts)\nyy = [float(y[0]) for y in loss_samples]\nlogy = [log(y) for y in yy]\nf, axes = plt.subplots(1,2)\nf.tight_layout()\naxes[0].hist(yy, bins=30, log=True)\naxes[0].set_title('log-Histogram of loss')\naxes[1].hist(logy, bins=30, log=False)\naxes[1].set_title('Histogram of log(loss)')\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["* As expected the loss values have a lognormal distribution tendency and would respond well for log transformed linear regression"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import log\n\ndataset = dataset.filter(dataset.loss > 1)                       #remove any data that would have negative log values\ndataset = dataset.withColumn(\"log_loss\",log(dataset.loss))\ndataset.select(\"loss\",\"log_loss\").show(5)\ndataset.select(dataset.columns[131:133]).describe().toPandas().transpose()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>summary</th>\n      <td>count</td>\n      <td>mean</td>\n      <td>stddev</td>\n      <td>min</td>\n      <td>max</td>\n    </tr>\n    <tr>\n      <th>loss</th>\n      <td>188317</td>\n      <td>3037.3538109676856</td>\n      <td>2904.085466237326</td>\n      <td>5.25</td>\n      <td>121012.25</td>\n    </tr>\n    <tr>\n      <th>log_loss</th>\n      <td>188317</td>\n      <td>7.685310779734525</td>\n      <td>0.8116564065242151</td>\n      <td>1.6582280766035324</td>\n      <td>11.70364705912391</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["#Let us look at the correlation between the continuous variables and the loss values\n\nfeatures = dataset.columns[117:132]\nprint('%s:   %19s\\t %s\\t    ' % ('feature','Loss','LogLoss'))\n\nfor feature in features:\n  print(\"Correlation of %6s : %6.3f\\t %6.3f: \" %(feature,dataset.stat.corr(feature, 'loss'),dataset.stat.corr(feature, 'log_loss')))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">feature:                  Loss\t LogLoss\t    \nCorrelation of  cont1 : -0.010\t -0.007: \nCorrelation of  cont2 :  0.142\t  0.105: \nCorrelation of  cont3 :  0.111\t  0.082: \nCorrelation of  cont4 : -0.036\t -0.027: \nCorrelation of  cont5 : -0.011\t -0.015: \nCorrelation of  cont6 :  0.041\t  0.032: \nCorrelation of  cont7 :  0.120\t  0.085: \nCorrelation of  cont8 :  0.031\t  0.032: \nCorrelation of  cont9 :  0.014\t  0.017: \nCorrelation of cont10 :  0.020\t  0.011: \nCorrelation of cont11 :  0.100\t  0.073: \nCorrelation of cont12 :  0.099\t  0.072: \nCorrelation of cont13 : -0.004\t  0.003: \nCorrelation of cont14 :  0.019\t  0.026: \n</div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["* There doesn't seem to be a strong correlation between individual variables to loss . \n* cont2, cont3 and cont7 are the ones most strongly correlated to loss."],"metadata":{}},{"cell_type":"code","source":["#plotting the continuous data against loss might give us further insight\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = dataset.columns[117:131]\nnfeatures = len(features)\nrowlength = 3\nlastrowlength = nfeatures % rowlength\nnrows = int(np.ceil(float(nfeatures)/rowlength))\n\nnpts = 400\n\nf, axes_all = plt.subplots(nrows, rowlength, sharey=True, figsize=(10,10))\nf.tight_layout()\nloss_samples = dataset.select('loss').take(npts)\nyy = [y for y in loss_samples]\n\nfor irow in range(nrows):\n  if (irow == nrows-1):\n    thisrowlength = lastrowlength\n  else:\n    thisrowlength = rowlength\n  first = rowlength*irow\n  last = min(rowlength*(irow+1),nfeatures)\n  feats = features[first:last]\n  for iplot in range(thisrowlength):\n    data = dataset.select(feats[iplot]).take(npts)\n    xx = [x for x in data]    \n    axes_all[irow][iplot].scatter(xx,yy,s=10,alpha=0.4)\n    axes_all[irow][iplot].set_xlabel(feats[iplot], fontsize='medium')\n    axes_all[irow][iplot].set_ylabel('loss')\n    axes_all[irow][iplot].get_yaxis().set_ticks([])\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["* There is again no significant trend between continuous variables and the loss data\n* cont2 variable seems to have been binned and has discrete values"],"metadata":{}},{"cell_type":"markdown","source":["<h3>Data Preparation</h3>"],"metadata":{}},{"cell_type":"code","source":["#Let us first convert the categorical data into numerical. Here, the most frequent category gets the lowest index. \n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\n\ndf = dataset\nPL1 = Pipeline(stages=[\n    StringIndexer(inputCol=c, outputCol='{}_Ind'.format(c)).setHandleInvalid(\"error\") for c in df.columns if c.startswith(\"cat\") \n])\n\ndf_indexed = PL1.fit(df).transform(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["#Let us check the feature size of each categorical data column\nnCategory ={}\nheaders = df_indexed.schema.names\n\nfor h in headers:  \n  if h.endswith(\"_Ind\"):  nCategory[h] = df_indexed.select(h).distinct().count()\nprint(nCategory)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;cat1_Ind&#39;: 2, &#39;cat2_Ind&#39;: 2, &#39;cat3_Ind&#39;: 2, &#39;cat4_Ind&#39;: 2, &#39;cat5_Ind&#39;: 2, &#39;cat6_Ind&#39;: 2, &#39;cat7_Ind&#39;: 2, &#39;cat8_Ind&#39;: 2, &#39;cat9_Ind&#39;: 2, &#39;cat10_Ind&#39;: 2, &#39;cat11_Ind&#39;: 2, &#39;cat12_Ind&#39;: 2, &#39;cat13_Ind&#39;: 2, &#39;cat14_Ind&#39;: 2, &#39;cat15_Ind&#39;: 2, &#39;cat16_Ind&#39;: 2, &#39;cat17_Ind&#39;: 2, &#39;cat18_Ind&#39;: 2, &#39;cat19_Ind&#39;: 2, &#39;cat20_Ind&#39;: 2, &#39;cat21_Ind&#39;: 2, &#39;cat22_Ind&#39;: 2, &#39;cat23_Ind&#39;: 2, &#39;cat24_Ind&#39;: 2, &#39;cat25_Ind&#39;: 2, &#39;cat26_Ind&#39;: 2, &#39;cat27_Ind&#39;: 2, &#39;cat28_Ind&#39;: 2, &#39;cat29_Ind&#39;: 2, &#39;cat30_Ind&#39;: 2, &#39;cat31_Ind&#39;: 2, &#39;cat32_Ind&#39;: 2, &#39;cat33_Ind&#39;: 2, &#39;cat34_Ind&#39;: 2, &#39;cat35_Ind&#39;: 2, &#39;cat36_Ind&#39;: 2, &#39;cat37_Ind&#39;: 2, &#39;cat38_Ind&#39;: 2, &#39;cat39_Ind&#39;: 2, &#39;cat40_Ind&#39;: 2, &#39;cat41_Ind&#39;: 2, &#39;cat42_Ind&#39;: 2, &#39;cat43_Ind&#39;: 2, &#39;cat44_Ind&#39;: 2, &#39;cat45_Ind&#39;: 2, &#39;cat46_Ind&#39;: 2, &#39;cat47_Ind&#39;: 2, &#39;cat48_Ind&#39;: 2, &#39;cat49_Ind&#39;: 2, &#39;cat50_Ind&#39;: 2, &#39;cat51_Ind&#39;: 2, &#39;cat52_Ind&#39;: 2, &#39;cat53_Ind&#39;: 2, &#39;cat54_Ind&#39;: 2, &#39;cat55_Ind&#39;: 2, &#39;cat56_Ind&#39;: 2, &#39;cat57_Ind&#39;: 2, &#39;cat58_Ind&#39;: 2, &#39;cat59_Ind&#39;: 2, &#39;cat60_Ind&#39;: 2, &#39;cat61_Ind&#39;: 2, &#39;cat62_Ind&#39;: 2, &#39;cat63_Ind&#39;: 2, &#39;cat64_Ind&#39;: 2, &#39;cat65_Ind&#39;: 2, &#39;cat66_Ind&#39;: 2, &#39;cat67_Ind&#39;: 2, &#39;cat68_Ind&#39;: 2, &#39;cat69_Ind&#39;: 2, &#39;cat70_Ind&#39;: 2, &#39;cat71_Ind&#39;: 2, &#39;cat72_Ind&#39;: 2, &#39;cat73_Ind&#39;: 3, &#39;cat74_Ind&#39;: 3, &#39;cat75_Ind&#39;: 3, &#39;cat76_Ind&#39;: 3, &#39;cat77_Ind&#39;: 4, &#39;cat78_Ind&#39;: 4, &#39;cat79_Ind&#39;: 4, &#39;cat80_Ind&#39;: 4, &#39;cat81_Ind&#39;: 4, &#39;cat82_Ind&#39;: 4, &#39;cat83_Ind&#39;: 4, &#39;cat84_Ind&#39;: 4, &#39;cat85_Ind&#39;: 4, &#39;cat86_Ind&#39;: 4, &#39;cat87_Ind&#39;: 4, &#39;cat88_Ind&#39;: 4, &#39;cat89_Ind&#39;: 8, &#39;cat90_Ind&#39;: 7, &#39;cat91_Ind&#39;: 8, &#39;cat92_Ind&#39;: 7, &#39;cat93_Ind&#39;: 5, &#39;cat94_Ind&#39;: 7, &#39;cat95_Ind&#39;: 5, &#39;cat96_Ind&#39;: 8, &#39;cat97_Ind&#39;: 7, &#39;cat98_Ind&#39;: 5, &#39;cat99_Ind&#39;: 16, &#39;cat100_Ind&#39;: 15, &#39;cat101_Ind&#39;: 19, &#39;cat102_Ind&#39;: 9, &#39;cat103_Ind&#39;: 13, &#39;cat104_Ind&#39;: 17, &#39;cat105_Ind&#39;: 20, &#39;cat106_Ind&#39;: 17, &#39;cat107_Ind&#39;: 20, &#39;cat108_Ind&#39;: 11, &#39;cat109_Ind&#39;: 84, &#39;cat110_Ind&#39;: 131, &#39;cat111_Ind&#39;: 16, &#39;cat112_Ind&#39;: 51, &#39;cat113_Ind&#39;: 61, &#39;cat114_Ind&#39;: 19, &#39;cat115_Ind&#39;: 23, &#39;cat116_Ind&#39;: 326}\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["The categorical data after cat109 indeed have more levels with cat116 having the mamimum feature size of 326. This knowledge will be useful while setting up the random forest regressor."],"metadata":{}},{"cell_type":"code","source":["# We'll need a list of headers for the columns we would like to assemble into a vector \nassemblerInputs =[]\n\nfor h in headers:  \n  if h.endswith(\"_Ind\") or h.startswith(\"cont\"): assemblerInputs.append(h)      \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["#Time to assemble the features vector and isolate the related columns for training\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(\n    inputCols= assemblerInputs,\n    outputCol=\"features\")\n\ndataVec = assembler.transform(df_indexed).select(\"features\",\"loss\")\ndataVec.show(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------+------------------+\n            features|   loss|          log_loss|\n+--------------------+-------+------------------+\n(130,[0,1,2,3,4,5...|2213.18| 7.702185674294166|\n(130,[0,1,2,3,4,5...| 1283.6|7.1574239092357015|\n(130,[0,1,2,3,4,5...|3005.09| 8.008062796604008|\n(130,[0,1,2,3,4,5...| 939.85| 6.845720288062604|\n(130,[0,1,2,3,4,5...|2763.85| 7.924379914239827|\n+--------------------+-------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["#Let us split the training data into two. \n\ntrainVec, testVec = dataVec.randomSplit(\n  [0.8, 0.2],                    # 80-20 split\n  seed= 5)                       # For reproducibility\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["<h3>Modeling</h3>\n\nWe'll start setting up a baseline. Here, the baseline prediction will be the mean of the population."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n\ndf_stats = dataVec.select(\n    _mean(col('loss')).alias('mean'),\n    _stddev(col('loss')).alias('std')\n).collect()\n\nmean = float(df_stats[0]['mean'])\nsigma = float(df_stats[0]['std'])\n\nprint(\"Mean: %.2f, Standard Deviation: %.2f\" %(mean,sigma))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Mean: 3037.35, Standard Deviation: 2904.09\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["from pyspark.sql.functions import lit\n\nbaselineDF = trainVec.withColumn(\"prediction\", lit(mean))\nbaselineDF.select(\"loss\",\"prediction\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+------------------+\n    loss|        prediction|\n+--------+------------------+\n 9425.19|3037.3538109676856|\n 5097.27|3037.3538109676856|\n 7747.11|3037.3538109676856|\n13588.22|3037.3538109676856|\n 7244.03|3037.3538109676856|\n+--------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["#Let us setup our evaluation metrics for all the models to be trained\n\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\nFOMs = []\n\ndef get_eval_metrics(model_name, predictDF, label_col, prediction_col):\n  \n  evaluator = RegressionEvaluator(labelCol=label_col, predictionCol=prediction_col)\n  mae = evaluator.evaluate(predictDF, {evaluator.metricName: \"mae\"})\n  rms = evaluator.evaluate(predictDF, {evaluator.metricName: \"rmse\"})\n  r2 = evaluator.evaluate(predictDF, {evaluator.metricName: \"r2\"})\n  print(\"%s MAE: %.3f, RMS: %.3f, R2: %.3f\" % (model_name,mae,rms,r2))\n\n  return [model_name,mae,rms,r2]\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["baseline_FOM = get_eval_metrics(\"Baseline (avg)\", baselineDF,\"loss\",\"prediction\")\nFOMs.append(baseline_FOM)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Baseline (avg) MAE: 1962.330, RMS: 2893.825, R2: -0.000\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["* The errors are bad as expected. Let us try to fit a bit more complicated models to get better results."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n\nlinR = LinearRegression(maxIter=25, regParam=0.01, solver=\"normal\", featuresCol='features', labelCol='loss')\nlinModel = linR.fit(trainVec)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["predictDF_lin = linModel.transform(testVec)\npredictDF_lin.select(\"loss\",\"prediction\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+\n   loss|        prediction|\n+-------+------------------+\n2537.11|2338.8517793354995|\n1367.49|2350.0442297470913|\n1767.21| 4556.500209342253|\n2985.81| 4639.248779146789|\n1143.94|2796.6794369839995|\n+-------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["linear_FOM = get_eval_metrics(\"Linear Regression\", predictDF_lin,\"loss\",\"prediction\")\nFOMs.append(linear_FOM)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Linear Regression MAE: 1370.263, RMS: 2163.083, R2: 0.460\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["* The results are significantly better but let us keep exploring\n* Maybe we can make use of the lognormal distribution of the loss data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import GeneralizedLinearRegression\n\nglr = GeneralizedLinearRegression(family=\"gaussian\", link=\"log\", labelCol='loss', featuresCol='features', maxIter=25, regParam = 0.01, linkPredictionCol= 'linkPrediction')\n\nGLRmodel = glr.fit(trainVec)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["predictDF_log = GLRmodel.transform(testVec)\npredictDF_log.select(\"loss\",\"prediction\",'linkPrediction').show(5)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+-----------------+\n   loss|        prediction|   linkPrediction|\n+-------+------------------+-----------------+\n2537.11|1730.1513637292578|7.455964177149613|\n1367.49| 2353.175796482641|7.763521097573919|\n1767.21| 3496.592924342285|8.159544323179245|\n2985.81|  3682.09726348224|8.211237777381728|\n1143.94| 2288.524647857548|7.735662630347058|\n+-------+------------------+-----------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["log_FOM = get_eval_metrics(\"Log Transform\", predictDF_log,\"loss\",\"prediction\")\nFOMs.append(log_FOM)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Log Transform MAE: 1391.214, RMS: 2146.954, R2: 0.468\n</div>"]}}],"execution_count":39},{"cell_type":"markdown","source":["* We see this model can explain the variance in the dataset better than the linear regression. Yet its MAE is worse. My guess is this model is optimized for RMS and hence has a better performance there. Yet, there are not that many parameters to tune over here and there is also no other option to change the loss function to at this high level.\n\n* Up next, we'll check the performance of the Random Forest Regression on our data. Since the parameter settings for RFR are less straightforward, we'll do a hyper parameter tuning"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder,CrossValidator \nfrom pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n\nmaxBins  = 326                        # Max num of branches to be created >= max num of levels in a category\nseed     = None                       # Keeping seeds random\nmaxDepth    = [10,15]                 # Branches won't be created further than this level\nminInfoGain = [0.2,0.5,1]             # Branches won't be created if their info gain is less than this\nnumTrees  = [20]                      # Number of trees in the forest to reduce overfitting\nnumFolds  = 10                        # Number of groups to divide the training data for cross-validation\n\nrfr = (RandomForestRegressor(featuresCol='features')\n      .setLabelCol(\"loss\")                  \n      .setMaxBins(maxBins)\n      .setSeed(seed) \n)\n\nevaluator = RegressionEvaluator(labelCol=\"loss\", predictionCol=\"prediction\")\n\nparamGrid = ParamGridBuilder() \\\n      .addGrid(rfr.numTrees, numTrees) \\\n      .addGrid(rfr.maxDepth, maxDepth) \\\n      .addGrid(rfr.minInfoGain, minInfoGain) \\\n      .build()\n\ncv = CrossValidator() \\\n      .setEstimator(rfr) \\\n      .setEvaluator(evaluator) \\\n      .setEstimatorParamMaps(paramGrid) \\\n      .setNumFolds(numFolds)\n\ncvModel = cv.fit(trainVec) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["predictDF_rfr = cvModel.transform(testVec)\npredictDF_rfr.select(\"loss\",\"prediction\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+\n   loss|        prediction|\n+-------+------------------+\n2537.11|1396.0888921571852|\n1367.49|2861.2177902978256|\n1767.21| 3351.729764780112|\n2985.81|   5047.8252247477|\n1143.94|2506.2366244158675|\n+-------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["rfr_FOM = get_eval_metrics(\"Random Forest\", predictDF_rfr,\"loss\",\"prediction\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Random Forest MAE: 1335.724, RMS: 2095.784, R2: 0.494\n</div>"]}}],"execution_count":43},{"cell_type":"code","source":["bestModel = cvModel.bestModel          #get the best model from cross-validation\nbestModel.extractParamMap()            #get the default and set values for all parameters\n#bestModel.explainParam(\"minInfoGain\")  #get the default and set values for a single parameter"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[138]: {Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;cacheNodeIds&#39;, doc=&#39;If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees.&#39;): False,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;checkpointInterval&#39;, doc=&#39;set checkpoint interval (&gt;= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext&#39;): 10,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;featureSubsetStrategy&#39;, doc=&#39;The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n].&#39;): &#39;auto&#39;,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;featuresCol&#39;, doc=&#39;features column name&#39;): &#39;features&#39;,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;impurity&#39;, doc=&#39;Criterion used for information gain calculation (case-insensitive). Supported options: variance&#39;): &#39;variance&#39;,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;labelCol&#39;, doc=&#39;label column name&#39;): &#39;loss&#39;,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;maxBins&#39;, doc=&#39;Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature.&#39;): 326,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;maxDepth&#39;, doc=&#39;Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.&#39;): 10,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;maxMemoryInMB&#39;, doc=&#39;Maximum memory in MB allocated to histogram aggregation.&#39;): 256,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;minInfoGain&#39;, doc=&#39;Minimum information gain for a split to be considered at a tree node.&#39;): 0.5,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;minInstancesPerNode&#39;, doc=&#39;Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1.&#39;): 1,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;numTrees&#39;, doc=&#39;Number of trees to train (at least 1)&#39;): 20,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;predictionCol&#39;, doc=&#39;prediction column name&#39;): &#39;prediction&#39;,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;seed&#39;, doc=&#39;random seed&#39;): None,\n Param(parent=&#39;RandomForestRegressor_5c0690cc46ae&#39;, name=&#39;subsamplingRate&#39;, doc=&#39;Fraction of the training data used for learning each decision tree, in range (0, 1].&#39;): 1.0}</div>"]}}],"execution_count":44},{"cell_type":"markdown","source":["Using the bestmodel parameters to avoid rerunning crossvalidation each time"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor, RandomForestRegressionModel\n\nmaxBins     = 326                     # Max num of branches to be created >= max num of levels in a category\nseed        = 42                      # Setting a seed for reproducibility\nmaxDepth    = 10                      # Branches won't be created further than this level\nminInfoGain = 0.5                     # Branches won't be created if their info gain is less than this\nnumTrees    = 20                      # Number of trees in the forest to reduce overfitting\n\nrfr = (RandomForestRegressor(featuresCol='features')\n      .setLabelCol(\"loss\")                  \n      .setMaxBins(maxBins)\n      .setSeed(seed) \n      .setMaxDepth(maxDepth)\n      .setMinInfoGain(minInfoGain)\n      .setNumTrees(numTrees)\n)\n\nRFRmodel = rfr.fit(trainVec)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":46},{"cell_type":"code","source":["predictDF_rfr = RFRmodel.transform(testVec)\npredictDF_rfr.select(\"loss\",\"prediction\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+------------------+\n   loss|        prediction|\n+-------+------------------+\n2537.11|1552.2343819423377|\n1367.49|2010.5620776012206|\n1767.21| 3477.605256020511|\n2985.81| 4877.342992871096|\n1143.94|  2453.68042317675|\n+-------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["rfr_FOM = get_eval_metrics(\"Random Forest\", predictDF_rfr,\"loss\",\"prediction\")\nFOMs.append(rfr_FOM)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Random Forest MAE: 1329.432, RMS: 2083.591, R2: 0.499\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["  print( \"%-20s %-10s %-10s %-10s\"  %(\"Model\",\"MAE\",\"RMS\",\"R2\"))\n  \n  for [name,mae,rms,r2] in FOMs:\n    print( \"%-20s %-10.2f %-10.2f %-10.2f\"  %(name,mae,rms,r2))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model                MAE        RMS        R2        \nBaseline (avg)       1962.33    2893.82    -0.00     \nLinear Regression    1370.26    2163.08    0.46      \nLog Transform        1391.21    2146.95    0.47      \nRandom Forest        1329.43    2083.59    0.50      \n</div>"]}}],"execution_count":49},{"cell_type":"markdown","source":["The random forest seems to be doing much better compared to the rest of the models, partly due to the hyper parameter tuning. \nLet us look into a bit more in detail to the model results."],"metadata":{}},{"cell_type":"code","source":["npts = 1000\ndef sample_list(predictDF,npts):\n  results = predictDF.select('loss', 'prediction').take(npts)\n  loss    = [r['loss'] for r in results]\n  abs_err = [r['prediction']-r['loss'] for r in results]\n  rel_err = [abs(r['prediction']-r['loss'])/r['loss'] for r in results]  \n  return loss,abs_err,rel_err\n  \n[loss_base,abs_err_base,rel_err_base] = sample_list(baselineDF,npts)\n[loss_lin,abs_err_lin,rel_err_lin] = sample_list(predictDF_lin,npts)\n[loss_log,abs_err_log,rel_err_log] = sample_list(predictDF_log,npts)\n[loss_rfr,abs_err_rfr,rel_err_rfr] = sample_list(predictDF_rfr,npts)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":51},{"cell_type":"code","source":["f, axes = plt.subplots(1,3,figsize=(25,8))\nplot_base = axes[0].scatter(loss_base,abs_err_base,s=8,c='red',linewidth=0,alpha=0.7)\nplot_lin = axes[0].scatter(loss_lin,abs_err_lin,s=8,c='blue',linewidth=0,alpha=0.7)\naxes[0].legend((plot_base,plot_lin),('Baseline','Linear'))\naxes[0].set_xlabel('Loss')\naxes[0].set_ylabel('Error')\n\nplot_base = axes[1].scatter(loss_base,abs_err_base,s=8,c='red',linewidth=0,alpha=0.7)\nplot_lin = axes[1].scatter(loss_lin,abs_err_lin,s=8,c='blue',linewidth=0,alpha=0.7)\nplot_log = axes[1].scatter(loss_log,abs_err_log,s=8,c='green',linewidth=0,alpha=0.7)\naxes[1].set_xlabel('Loss')\naxes[1].set_ylabel('Error')\naxes[1].legend((plot_base,plot_lin,plot_log),('Baseline','Linear','Log'))\n\nplot_base = axes[2].scatter(loss_base,abs_err_base,s=8,c='red',linewidth=0,alpha=0.7)\nplot_lin = axes[2].scatter(loss_lin,abs_err_lin,s=8,c='blue',linewidth=0,alpha=0.7)\nplot_log = axes[2].scatter(loss_log,abs_err_log,s=8,c='green',linewidth=0,alpha=0.7)\nplot_rfr = axes[2].scatter(loss_rfr,abs_err_rfr,s=8,c='magenta',linewidth=0,alpha=0.7)\naxes[2].set_xlabel('Loss')\naxes[2].set_ylabel('Error')\naxes[2].legend((plot_base,plot_lin,plot_log,plot_rfr),('Baseline','Linear','Log','RFR'))\n\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["* The reduction in variance is visible from linear to log-transform and to random forest regression"],"metadata":{}},{"cell_type":"code","source":["f, axes = plt.subplots(1,2,figsize=(10,6))\n\nplot_base = axes[0].hist([rel_err_base,rel_err_lin,rel_err_log,rel_err_rfr], bins=20, log=False)\naxes[0].set_xlabel('Rel Error')\naxes[0].set_ylabel('Count')\naxes[0].set_xlim(0,3)\n#axes[0].set_ylim(0,50)\naxes[0].legend(('Baseline','Linear','Log','RFR'))\n\n\nplot_base = axes[1].hist([rel_err_base,rel_err_lin,rel_err_log,rel_err_rfr], bins=10, log=False)\naxes[1].set_xlabel('Rel Error')\naxes[1].set_ylabel('Count')\naxes[1].set_xlim(3,18)\naxes[1].set_ylim(0,60)\naxes[1].legend(('Baseline','Linear','Log','RFR'))\n\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":54}],"metadata":{"name":"Loss Prediction","notebookId":2144521525930970},"nbformat":4,"nbformat_minor":0}
